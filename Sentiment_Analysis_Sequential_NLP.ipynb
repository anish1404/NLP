{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis Sequential NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bQBVb2Crd3VV3rjYVTBnOhMADmPGMXQ2",
      "authorship_tag": "ABX9TyOodpbu2QboiO+7+Ti95m6o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anish1404/NLP/blob/main/Sentiment_Analysis_Sequential_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eLiK_vA0lN2"
      },
      "source": [
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1DYr6Z607iO",
        "outputId": "8fc7ffd5-dc39-4c74-e98b-6e3c4456e723"
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfY4t7nF1Bd8",
        "outputId": "ea4ab4bc-63b7-40eb-ef93-f243e20a299c"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLq-IGIU1Fxi",
        "outputId": "0f0e2f1c-f050-4e75-a3fc-43af6163b379"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8P1KlXS1H7D",
        "outputId": "7d5a86f1-ea64-47c6-ea43-4abc0a421649"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
              "       list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 2, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 1074, 5, 1987, 2, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 2, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 2, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
              "       list([1, 111, 748, 4368, 1133, 2, 2, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 2, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 2, 589, 8, 22, 107, 2, 2, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 2, 2425, 34, 2, 8738, 2, 5, 2, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 2, 1060, 63, 29, 93, 11, 5421, 11, 2, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 2, 10, 10, 4, 993, 2, 7, 4, 1766, 2634, 2164, 2, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 2, 16, 6, 465, 993, 2006, 2, 573, 17, 2, 42, 4, 2, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 2, 146, 655, 2212, 5, 258, 12, 184, 2, 546, 5, 849, 2, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 2, 5, 2, 11, 661, 8, 339, 2, 4, 2455, 2, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 2, 2, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 2, 2, 18, 6, 711, 4, 9909, 26, 2, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 2, 18, 27, 173, 9, 2399, 17, 6, 2, 428, 2, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 2, 54, 2, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 2, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 2, 3292, 98, 6, 2, 10, 10, 6639, 19, 14, 2, 267, 162, 711, 37, 5900, 752, 98, 4, 2, 2378, 90, 19, 6, 2, 7, 2, 1810, 2, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 2, 17, 2, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 2, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 2, 5437, 33, 1526, 6, 425, 3155, 2, 4535, 1636, 7, 4, 4669, 2, 469, 4, 4552, 54, 4, 150, 5664, 2, 280, 53, 2, 2, 18, 339, 29, 1978, 27, 7885, 5, 2, 68, 1830, 19, 6571, 2, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 2, 33, 4, 5673, 7, 15, 2, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 6676, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 2, 787, 7, 2460, 2, 2, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 2, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
              "       ...,\n",
              "       list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 2, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 2, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
              "       list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
              "       list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 2, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 2, 21, 4, 7298, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 5698, 3406, 718, 2, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QfgFIhZ1d8Z",
        "outputId": "8648f21d-c91b-41bc-f346-c1e798d15331"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W9JPqIO1ypB"
      },
      "source": [
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dkegCEh2Jkj",
        "outputId": "03fb3e2e-2dac-4df6-9688-bee1afdd00a9"
      },
      "source": [
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 32)           320000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 373,301\n",
            "Trainable params: 373,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 292s 698ms/step - loss: 0.5610 - accuracy: 0.6901 - val_loss: 0.3151 - val_accuracy: 0.8657\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 274s 700ms/step - loss: 0.2408 - accuracy: 0.9078 - val_loss: 0.3419 - val_accuracy: 0.8543\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 273s 699ms/step - loss: 0.1899 - accuracy: 0.9333 - val_loss: 0.3845 - val_accuracy: 0.8624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa27a21ce90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gc5KZd97T6p"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "\n",
        "for x in range(len(X_test)):\n",
        "    \n",
        "    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.argmax(result) == np.argmax(Y_test[x]):\n",
        "        if np.argmax(Y_test[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if np.argmax(Y_test[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_4AulsD56Uv",
        "outputId": "6b0e6840-2964-4878-c9a7-8f392b5abba2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "vocabSize = 5000\n",
        "tokenizer = Tokenizer(num_words=vocabSize, split=' ')\n",
        "twt = ['He is a lazy person.']\n",
        "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
        "twt =  tokenizer.texts_to_sequences(twt)\n",
        "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
        "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
        "print(twt)\n",
        "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
        "if(np.argmax(sentiment) == 0):\n",
        "    print(\"negative\")\n",
        "elif (np.argmax(sentiment) == 1):\n",
        "    print(\"positive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "1/1 - 0s\n",
            "negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Quw4XZKkVN"
      },
      "source": [
        "**PART 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9wDU1D8NGT5"
      },
      "source": [
        "from keras.layers import Embedding, Flatten, Dense, LSTM, Bidirectional, Dropout, Conv1D, MaxPool1D\n",
        "from keras.layers import GlobalMaxPool1D, GRU\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hq9EN7SC6ypL",
        "outputId": "2704d5cf-115f-4c8c-f402-4919268d0875"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df=pd.read_json(\"/content/drive/MyDrive/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        article_link  ... is_sarcastic\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...  ...            0\n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...  ...            0\n",
              "2  https://local.theonion.com/mom-starting-to-fea...  ...            1\n",
              "3  https://politics.theonion.com/boehner-just-wan...  ...            1\n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...  ...            0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44lYoiXRK0IS",
        "outputId": "fd0df847-8672-4fd2-e479-d77119f27335"
      },
      "source": [
        "df['is_sarcastic'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    14985\n",
              "1    11724\n",
              "Name: is_sarcastic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5kFkW3OK8an"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X=df['headline'].values\n",
        "y=df['is_sarcastic'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65t7E6QSLB7Y",
        "outputId": "3cb0a1e4-5693-4944-f5ed-c0974f80e837"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18696,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2QowaxjLD-7",
        "outputId": "382d03e1-3196-440b-bbbd-76dcc1d98acf"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8013,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB1u_ek1LGNt"
      },
      "source": [
        "vocab_size=10000\n",
        "embedding_dim=16\n",
        "max_length=32\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok='<oov>'\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer= Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index=tokenizer.word_index\n",
        "training_sequences=tokenizer.texts_to_sequences(X_train)\n",
        "training_padded=pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "testing_sequences=tokenizer.texts_to_sequences(X_test)\n",
        "testing_padded=pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwxbquGDLdRt",
        "outputId": "3b0892ec-ca22-4385-d927-0fc76534d533"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=10,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=1,activation='sigmoid'))\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 32, 16)            160000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 176,757\n",
            "Trainable params: 176,757\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKOrhmD0LlY9",
        "outputId": "847ff35f-0f27-47f0-c671-ba53c8fd5c25"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "model.fit(x=training_padded, y=y_train, batch_size=256, epochs=100, validation_data=(testing_padded, y_test), verbose=1, callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.6894 - accuracy: 0.5438 - val_loss: 0.6873 - val_accuracy: 0.5546\n",
            "Epoch 2/100\n",
            "74/74 [==============================] - 0s 6ms/step - loss: 0.6855 - accuracy: 0.5618 - val_loss: 0.6846 - val_accuracy: 0.5546\n",
            "Epoch 3/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.6841 - accuracy: 0.5623 - val_loss: 0.6823 - val_accuracy: 0.5546\n",
            "Epoch 4/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.6795 - accuracy: 0.5637 - val_loss: 0.6777 - val_accuracy: 0.5546\n",
            "Epoch 5/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.6716 - accuracy: 0.5696 - val_loss: 0.6677 - val_accuracy: 0.5550\n",
            "Epoch 6/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.6587 - accuracy: 0.5871 - val_loss: 0.6505 - val_accuracy: 0.5726\n",
            "Epoch 7/100\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.6387 - accuracy: 0.6172 - val_loss: 0.6256 - val_accuracy: 0.6438\n",
            "Epoch 8/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.6111 - accuracy: 0.6591 - val_loss: 0.5917 - val_accuracy: 0.7065\n",
            "Epoch 9/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.5773 - accuracy: 0.7046 - val_loss: 0.5531 - val_accuracy: 0.7777\n",
            "Epoch 10/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.5408 - accuracy: 0.7410 - val_loss: 0.5158 - val_accuracy: 0.8004\n",
            "Epoch 11/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.5078 - accuracy: 0.7761 - val_loss: 0.4839 - val_accuracy: 0.8279\n",
            "Epoch 12/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.4783 - accuracy: 0.7993 - val_loss: 0.4564 - val_accuracy: 0.8331\n",
            "Epoch 13/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.4494 - accuracy: 0.8191 - val_loss: 0.4324 - val_accuracy: 0.8400\n",
            "Epoch 14/100\n",
            "74/74 [==============================] - 0s 6ms/step - loss: 0.4237 - accuracy: 0.8350 - val_loss: 0.4137 - val_accuracy: 0.8433\n",
            "Epoch 15/100\n",
            "74/74 [==============================] - 0s 6ms/step - loss: 0.4051 - accuracy: 0.8471 - val_loss: 0.3934 - val_accuracy: 0.8521\n",
            "Epoch 16/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.3812 - accuracy: 0.8555 - val_loss: 0.3804 - val_accuracy: 0.8544\n",
            "Epoch 17/100\n",
            "74/74 [==============================] - 0s 6ms/step - loss: 0.3596 - accuracy: 0.8681 - val_loss: 0.3684 - val_accuracy: 0.8571\n",
            "Epoch 18/100\n",
            "74/74 [==============================] - 0s 6ms/step - loss: 0.3431 - accuracy: 0.8805 - val_loss: 0.3587 - val_accuracy: 0.8586\n",
            "Epoch 19/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.3236 - accuracy: 0.8896 - val_loss: 0.3492 - val_accuracy: 0.8595\n",
            "Epoch 20/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.3082 - accuracy: 0.8974 - val_loss: 0.3448 - val_accuracy: 0.8580\n",
            "Epoch 21/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.2959 - accuracy: 0.9035 - val_loss: 0.3389 - val_accuracy: 0.8621\n",
            "Epoch 22/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.2807 - accuracy: 0.9113 - val_loss: 0.3353 - val_accuracy: 0.8612\n",
            "Epoch 23/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.2725 - accuracy: 0.9146 - val_loss: 0.3315 - val_accuracy: 0.8622\n",
            "Epoch 24/100\n",
            "74/74 [==============================] - 0s 6ms/step - loss: 0.2565 - accuracy: 0.9216 - val_loss: 0.3317 - val_accuracy: 0.8626\n",
            "Epoch 25/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.2478 - accuracy: 0.9296 - val_loss: 0.3298 - val_accuracy: 0.8633\n",
            "Epoch 26/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.2362 - accuracy: 0.9304 - val_loss: 0.3282 - val_accuracy: 0.8637\n",
            "Epoch 27/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.2256 - accuracy: 0.9360 - val_loss: 0.3282 - val_accuracy: 0.8641\n",
            "Epoch 28/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.2182 - accuracy: 0.9415 - val_loss: 0.3290 - val_accuracy: 0.8643\n",
            "Epoch 29/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.2051 - accuracy: 0.9447 - val_loss: 0.3287 - val_accuracy: 0.8646\n",
            "Epoch 30/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.2023 - accuracy: 0.9465 - val_loss: 0.3341 - val_accuracy: 0.8636\n",
            "Epoch 31/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1944 - accuracy: 0.9499 - val_loss: 0.3273 - val_accuracy: 0.8637\n",
            "Epoch 32/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1862 - accuracy: 0.9538 - val_loss: 0.3373 - val_accuracy: 0.8641\n",
            "Epoch 33/100\n",
            "74/74 [==============================] - 0s 6ms/step - loss: 0.1732 - accuracy: 0.9569 - val_loss: 0.3322 - val_accuracy: 0.8637\n",
            "Epoch 34/100\n",
            "74/74 [==============================] - 1s 7ms/step - loss: 0.1689 - accuracy: 0.9581 - val_loss: 0.3353 - val_accuracy: 0.8640\n",
            "Epoch 35/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1643 - accuracy: 0.9595 - val_loss: 0.3454 - val_accuracy: 0.8636\n",
            "Epoch 36/100\n",
            "74/74 [==============================] - 0s 7ms/step - loss: 0.1597 - accuracy: 0.9644 - val_loss: 0.3440 - val_accuracy: 0.8655\n",
            "Epoch 00036: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa271e1f650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGisNlfLLppE"
      },
      "source": [
        "\n",
        "def lstm():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(input_dim=20000, output_dim=6, input_length=40))\n",
        "    \n",
        "    model.add(Bidirectional(LSTM(16, return_sequences=True, recurrent_dropout=0.1, dropout=0.1)))\n",
        "    \n",
        "    model.add(Bidirectional(LSTM(32, recurrent_dropout=0.1, dropout=0.1)))\n",
        "    \n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(optimizer = 'rmsprop',\n",
        "                  loss = 'binary_crossentropy',\n",
        "                  metrics = ['acc'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "lsmod = lstm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFUs2bJyM3rw",
        "outputId": "736373e2-9e12-41e1-8a7a-6ba275fd6005"
      },
      "source": [
        "# Train the model\n",
        "lshist = lsmod.fit(training_padded, y_train,\n",
        "         epochs = 10,\n",
        "         batch_size = 256,\n",
        "         validation_data = (testing_padded, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 28s 227ms/step - loss: 0.6055 - acc: 0.6448 - val_loss: 0.6066 - val_acc: 0.7195\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 16s 216ms/step - loss: 0.3429 - acc: 0.8537 - val_loss: 0.3930 - val_acc: 0.8409\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 16s 214ms/step - loss: 0.2763 - acc: 0.8887 - val_loss: 0.6305 - val_acc: 0.7745\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 16s 214ms/step - loss: 0.2402 - acc: 0.9036 - val_loss: 0.4557 - val_acc: 0.8288\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 16s 213ms/step - loss: 0.2121 - acc: 0.9167 - val_loss: 0.3843 - val_acc: 0.8541\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 16s 212ms/step - loss: 0.1899 - acc: 0.9266 - val_loss: 0.4705 - val_acc: 0.8222\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 16s 216ms/step - loss: 0.1729 - acc: 0.9358 - val_loss: 0.4640 - val_acc: 0.8265\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 16s 214ms/step - loss: 0.1574 - acc: 0.9404 - val_loss: 0.4556 - val_acc: 0.8459\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 16s 216ms/step - loss: 0.1432 - acc: 0.9467 - val_loss: 0.4279 - val_acc: 0.8490\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 16s 216ms/step - loss: 0.1351 - acc: 0.9500 - val_loss: 0.4120 - val_acc: 0.8486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm4XhQk7N2Lz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrkgf9T8T9ud"
      },
      "source": [
        "**Using Glove Vectors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drZxHjXlPt7P"
      },
      "source": [
        "EMBEDDING_FILE = '/content/drive/MyDrive/glove.6B.200d.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr8dtXBBQE3W"
      },
      "source": [
        "def get_coefs(word, *arr): \n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxD6bMYYQHv1",
        "outputId": "69eccfca-77d1-46e8-985e-71b7372d74be"
      },
      "source": [
        "max_features=20000\n",
        "\n",
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "embed_size = all_embs.shape[1]\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "#change below line if computing normal stats is too slow\n",
        "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07d7T7BEQo6G",
        "outputId": "85c3ef89-c11a-4505-c78f-7b2d7379facc"
      },
      "source": [
        "import keras\n",
        "#Defining Neural Network\n",
        "model = Sequential()\n",
        "#Non-trainable embeddidng layer\n",
        "model.add(Embedding(nb_words, output_dim=embed_size, weights=[embedding_matrix], input_length=200, trainable=True))\n",
        "#LSTM \n",
        "model.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.5 , dropout = 0.5)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6xAkQUvQxsN",
        "outputId": "84a156e6-7066-419a-d552-f1a33b4f08e4"
      },
      "source": [
        "model.fit(training_padded, y_train,\n",
        "         epochs = 10,\n",
        "         batch_size = 256,\n",
        "         validation_data = (testing_padded, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 81s 1s/step - loss: 0.5233 - acc: 0.7448 - val_loss: 0.3425 - val_acc: 0.8520\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 75s 1s/step - loss: 0.2399 - acc: 0.9017 - val_loss: 0.3299 - val_acc: 0.8633\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 75s 1s/step - loss: 0.1380 - acc: 0.9484 - val_loss: 0.3867 - val_acc: 0.8585\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 74s 1s/step - loss: 0.0934 - acc: 0.9666 - val_loss: 0.4369 - val_acc: 0.8604\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 74s 1s/step - loss: 0.0617 - acc: 0.9782 - val_loss: 0.5537 - val_acc: 0.8561\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 74s 1s/step - loss: 0.0447 - acc: 0.9839 - val_loss: 0.6072 - val_acc: 0.8549\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 74s 1s/step - loss: 0.0402 - acc: 0.9865 - val_loss: 0.7014 - val_acc: 0.8521\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 74s 999ms/step - loss: 0.0287 - acc: 0.9908 - val_loss: 0.7416 - val_acc: 0.8536\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 74s 999ms/step - loss: 0.0229 - acc: 0.9922 - val_loss: 0.8022 - val_acc: 0.8480\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 74s 1s/step - loss: 0.0201 - acc: 0.9928 - val_loss: 0.8879 - val_acc: 0.8499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa25526b790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5iV1JnFUjCV"
      },
      "source": [
        "**acc: 0.9666 - val_loss: 0.4369 - val_acc: 0.8604 can be achieved**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tvDpAUxUs46"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}